{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SuperScan Demo: End-to-End Workflow\n",
    "\n",
    "This notebook demonstrates the complete SuperScan flow:\n",
    "\n",
    "1. **Initialize Database** - Create tables in Snowflake\n",
    "2. **Create Project** - Set up a new project container\n",
    "3. **Upload PDF** - Ingest a document and extract metadata\n",
    "4. **Sparse Scan** - Generate ontology proposal from PDF text\n",
    "5. **Review Proposal** - Inspect LLM-generated schema suggestions\n",
    "6. **Finalize Schemas** - Convert proposal into versioned Schema records\n",
    "\n",
    "**Requirements**: Snowflake credentials in `.env`, OpenAI API key (optional for LLM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent.parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Load environment\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(project_root / \".env\")\n",
    "\n",
    "print(\"‚úì Environment loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import core services\n",
    "from code.graph_rag.db import get_db, init_database\n",
    "from code.superscan.project_service import ProjectService\n",
    "from code.superscan.file_service import FileService\n",
    "from code.superscan.schema_service import SchemaService\n",
    "from code.superscan.proposal_service import ProposalService\n",
    "from code.superscan.fast_scan import FastScan\n",
    "from code.superscan.pdf_parser import extract_text_from_file_path\n",
    "\n",
    "print(\"‚úì Services imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Initialize Database\n",
    "\n",
    "Create all required tables in Snowflake (projects, schemas, nodes, edges, files, ontology_proposals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize database (creates tables if not exist)\n",
    "init_database()\n",
    "print(\"‚úì Database initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create Project\n",
    "\n",
    "Create a project container for our knowledge graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize services\n",
    "db = get_db()\n",
    "project_svc = ProjectService(db)\n",
    "file_svc = FileService(db)\n",
    "schema_svc = SchemaService(db)\n",
    "proposal_svc = ProposalService(db)\n",
    "\n",
    "# Create project\n",
    "project_payload = {\n",
    "    \"project_name\": \"demo-superscan\",\n",
    "    \"display_name\": \"SuperScan Demo Project\",\n",
    "    \"owner_id\": \"demo-user\",\n",
    "    \"tags\": [\"demo\", \"superscan\"],\n",
    "}\n",
    "\n",
    "project = project_svc.create_project(project_payload)\n",
    "project_id = project[\"project_id\"]\n",
    "print(f\"‚úì Project created: {project_id}\")\n",
    "print(f\"  Name: {project['project_name']}\")\n",
    "print(f\"  Status: {project['status']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Upload PDF\n",
    "\n",
    "Upload a sample PDF and store its metadata in Snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For demo: use a sample PDF or create a mock one\n",
    "# In production, this would be a real file upload\n",
    "\n",
    "sample_pdf_path = \"sample.pdf\"  # Replace with actual path\n",
    "\n",
    "# If no real PDF available, create mock metadata\n",
    "if not os.path.exists(sample_pdf_path):\n",
    "    print(\"‚ö†Ô∏è No sample PDF found. Using mock file metadata.\")\n",
    "    file_record = file_svc.upload_pdf(\n",
    "        project_id=project_id,\n",
    "        filename=\"sample_research_paper.pdf\",\n",
    "        size_bytes=1024000,  # 1 MB\n",
    "        pages=12,\n",
    "        metadata={\"source\": \"demo\", \"topic\": \"knowledge graphs\"},\n",
    "    )\n",
    "else:\n",
    "    # Real file upload\n",
    "    file_info = os.stat(sample_pdf_path)\n",
    "    extracted = extract_text_from_file_path(sample_pdf_path, max_pages=10)\n",
    "    \n",
    "    file_record = file_svc.upload_pdf(\n",
    "        project_id=project_id,\n",
    "        filename=os.path.basename(sample_pdf_path),\n",
    "        size_bytes=file_info.st_size,\n",
    "        pages=extracted.get(\"total_pages\", 0),\n",
    "        metadata={\"extracted_pages\": extracted.get(\"pages\", 0)},\n",
    "    )\n",
    "\n",
    "file_id = file_record[\"file_id\"]\n",
    "print(f\"‚úì File uploaded: {file_id}\")\n",
    "print(f\"  Filename: {file_record['filename']}\")\n",
    "print(f\"  Pages: {file_record['pages']}\")\n",
    "print(f\"  Status: {file_record['status']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Sparse Scan & Proposal Generation\n",
    "\n",
    "Extract sparse text from PDF and generate ontology proposal using LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract sparse text from PDF (or use mock data)\n",
    "if os.path.exists(sample_pdf_path):\n",
    "    extracted = extract_text_from_file_path(sample_pdf_path, max_pages=10)\n",
    "    text_snippets = extracted[\"text_snippets\"]\n",
    "else:\n",
    "    # Mock text snippets\n",
    "    text_snippets = [\n",
    "        \"This paper presents a novel approach to knowledge graph construction using multimodal databases.\",\n",
    "        \"We propose a schema-driven architecture that supports relational, graph, and vector representations.\",\n",
    "        \"The system enables entity resolution and deduplication across multiple data sources.\",\n",
    "    ]\n",
    "\n",
    "print(f\"‚úì Extracted {len(text_snippets)} text snippets\")\n",
    "print(f\"  Sample: {text_snippets[0][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate ontology proposal using LLM\n",
    "openai_key = os.getenv(\"OPENAI_API_KEY\")  # Optional\n",
    "scanner = FastScan(openai_api_key=openai_key)\n",
    "\n",
    "proposal_dict = scanner.generate_proposal(\n",
    "    snippets=text_snippets,\n",
    "    hints={\"domain\": \"knowledge graphs and databases\"},\n",
    ")\n",
    "\n",
    "print(\"\\n‚úì Ontology proposal generated\")\n",
    "print(f\"  Summary: {proposal_dict.get('summary', 'N/A')}\")\n",
    "print(f\"  Nodes: {len(proposal_dict.get('nodes', []))}\")\n",
    "print(f\"  Edges: {len(proposal_dict.get('edges', []))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Save Proposal to Snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save proposal to database\n",
    "proposal = proposal_svc.create_proposal(\n",
    "    project_id=project_id,\n",
    "    nodes=proposal_dict.get(\"nodes\", []),\n",
    "    edges=proposal_dict.get(\"edges\", []),\n",
    "    source_files=[file_id],\n",
    "    summary=proposal_dict.get(\"summary\", \"Ontology from sparse scan\"),\n",
    ")\n",
    "\n",
    "proposal_id = proposal[\"proposal_id\"]\n",
    "print(f\"‚úì Proposal saved: {proposal_id}\")\n",
    "print(f\"  Status: {proposal['status']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Review Proposal\n",
    "\n",
    "Inspect the generated ontology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "print(\"\\nüìã Proposal Details:\\n\")\n",
    "print(json.dumps(proposal, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Finalize Proposal ‚Üí Create Schemas\n",
    "\n",
    "Convert the proposal into versioned Schema records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finalize proposal (creates Schema records)\n",
    "result = proposal_svc.finalize_proposal(proposal_id)\n",
    "\n",
    "print(\"‚úì Proposal finalized. Schemas created:\")\n",
    "for schema in result[\"schemas\"]:\n",
    "    print(f\"  - {schema['schema_name']} (v{schema['version']}, {schema['entity_type']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Verify Schemas in Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all schemas for the project\n",
    "schemas = schema_svc.list_schemas(project_id)\n",
    "\n",
    "print(f\"\\n‚úì Project has {schemas['total']} schema(s):\")\n",
    "for s in schemas[\"items\"]:\n",
    "    print(f\"  - {s['schema_name']} v{s['version']} ({s['entity_type']}) [active={s['is_active']}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**SuperScan workflow completed successfully!**\n",
    "\n",
    "‚úÖ Project created  \n",
    "‚úÖ PDF uploaded and metadata stored  \n",
    "‚úÖ Sparse scan generated ontology proposal  \n",
    "‚úÖ Proposal saved to Snowflake  \n",
    "‚úÖ Schemas finalized and versioned  \n",
    "\n",
    "**Next steps (SuperKB)**:\n",
    "- Deep scan with chunking\n",
    "- Entity extraction and resolution\n",
    "- Embedding generation\n",
    "- Export to Postgres/Neo4j/Pinecone"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
